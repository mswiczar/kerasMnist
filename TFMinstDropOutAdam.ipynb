{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFMinstDropOutAdam.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Eo0QEANThi",
        "colab_type": "code",
        "outputId": "1cb8fbd8-57bd-4fce-cada-cbe0d9744b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0rc2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0rc0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (46.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo6FJCk6delC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZZ-oZVpOGUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "# Network and training.\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10   # number of outputs = number of digits\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
        "DROPOUT = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjBA3ZuOQJN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading MNIST dataset.\n",
        "   # verify\n",
        "   # You can verify that the split between train and test is 60,000, and 10,000 respectively.\n",
        "   # Labels have one-hot representation.is automatically applied\n",
        "mnist = keras.datasets.mnist\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaEkISokQ2va",
        "colab_type": "code",
        "outputId": "e2266523-ba68-4349-f390-0ce6de68ee09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObvnlaA1Q54f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
        "RESHAPED = 784\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z54XyZk3RIeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize inputs to be within in [0, 1].\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3y60knLRJtP",
        "colab_type": "code",
        "outputId": "175b7fde-6e3e-4c14-804e-6039e0045a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(X_train.shape[0], 'train samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFdS6m-mRNIV",
        "colab_type": "code",
        "outputId": "bb97688c-fb07-4f5a-e168-6dd30f46bd86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(X_test.shape[0], 'test samples')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQTTCbPRRRwD",
        "colab_type": "code",
        "outputId": "670c191f-ef3a-472c-960b-71008c4ccf22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "# Building the model.\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "             input_shape=(RESHAPED,),\n",
        "             name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "             name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "             name='dense_layer_3', activation='softmax'))\n",
        "   # Summary of the model.\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoT-OpkuRjVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model.\n",
        "model.compile(optimizer='Adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YH2DwY8RwUi",
        "colab_type": "code",
        "outputId": "56b9b2a3-5c96-424e-abbd-d5b324cfe863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training the model.\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.8447 - val_loss: 0.1913 - val_accuracy: 0.9442\n",
            "Epoch 2/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2366 - accuracy: 0.9295 - val_loss: 0.1419 - val_accuracy: 0.9569\n",
            "Epoch 3/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1782 - accuracy: 0.9467 - val_loss: 0.1156 - val_accuracy: 0.9647\n",
            "Epoch 4/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1500 - accuracy: 0.9557 - val_loss: 0.1031 - val_accuracy: 0.9703\n",
            "Epoch 5/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1308 - accuracy: 0.9605 - val_loss: 0.1024 - val_accuracy: 0.9693\n",
            "Epoch 6/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1168 - accuracy: 0.9646 - val_loss: 0.0984 - val_accuracy: 0.9703\n",
            "Epoch 7/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1073 - accuracy: 0.9664 - val_loss: 0.0908 - val_accuracy: 0.9753\n",
            "Epoch 8/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0960 - accuracy: 0.9692 - val_loss: 0.0933 - val_accuracy: 0.9732\n",
            "Epoch 9/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0914 - accuracy: 0.9719 - val_loss: 0.0902 - val_accuracy: 0.9749\n",
            "Epoch 10/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0833 - accuracy: 0.9736 - val_loss: 0.0863 - val_accuracy: 0.9767\n",
            "Epoch 11/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0817 - accuracy: 0.9744 - val_loss: 0.0852 - val_accuracy: 0.9772\n",
            "Epoch 12/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0747 - accuracy: 0.9760 - val_loss: 0.0824 - val_accuracy: 0.9762\n",
            "Epoch 13/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0713 - accuracy: 0.9764 - val_loss: 0.0829 - val_accuracy: 0.9772\n",
            "Epoch 14/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0650 - accuracy: 0.9790 - val_loss: 0.0834 - val_accuracy: 0.9767\n",
            "Epoch 15/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0644 - accuracy: 0.9795 - val_loss: 0.0823 - val_accuracy: 0.9768\n",
            "Epoch 16/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0612 - accuracy: 0.9804 - val_loss: 0.0803 - val_accuracy: 0.9787\n",
            "Epoch 17/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0589 - accuracy: 0.9814 - val_loss: 0.0835 - val_accuracy: 0.9774\n",
            "Epoch 18/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0606 - accuracy: 0.9802 - val_loss: 0.0832 - val_accuracy: 0.9782\n",
            "Epoch 19/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0564 - accuracy: 0.9816 - val_loss: 0.0787 - val_accuracy: 0.9786\n",
            "Epoch 20/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0553 - accuracy: 0.9816 - val_loss: 0.0808 - val_accuracy: 0.9786\n",
            "Epoch 21/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0509 - accuracy: 0.9833 - val_loss: 0.0871 - val_accuracy: 0.9785\n",
            "Epoch 22/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0494 - accuracy: 0.9838 - val_loss: 0.0843 - val_accuracy: 0.9793\n",
            "Epoch 23/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0510 - accuracy: 0.9833 - val_loss: 0.0896 - val_accuracy: 0.9772\n",
            "Epoch 24/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0476 - accuracy: 0.9846 - val_loss: 0.0905 - val_accuracy: 0.9788\n",
            "Epoch 25/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0486 - accuracy: 0.9838 - val_loss: 0.0852 - val_accuracy: 0.9788\n",
            "Epoch 26/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0441 - accuracy: 0.9851 - val_loss: 0.0865 - val_accuracy: 0.9800\n",
            "Epoch 27/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0463 - accuracy: 0.9844 - val_loss: 0.0834 - val_accuracy: 0.9794\n",
            "Epoch 28/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0418 - accuracy: 0.9860 - val_loss: 0.0882 - val_accuracy: 0.9783\n",
            "Epoch 29/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0430 - accuracy: 0.9857 - val_loss: 0.0913 - val_accuracy: 0.9785\n",
            "Epoch 30/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0433 - accuracy: 0.9854 - val_loss: 0.0894 - val_accuracy: 0.9782\n",
            "Epoch 31/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0385 - accuracy: 0.9868 - val_loss: 0.0858 - val_accuracy: 0.9800\n",
            "Epoch 32/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0422 - accuracy: 0.9861 - val_loss: 0.0861 - val_accuracy: 0.9787\n",
            "Epoch 33/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0390 - accuracy: 0.9868 - val_loss: 0.0868 - val_accuracy: 0.9802\n",
            "Epoch 34/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0403 - accuracy: 0.9866 - val_loss: 0.0839 - val_accuracy: 0.9807\n",
            "Epoch 35/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0389 - accuracy: 0.9866 - val_loss: 0.0871 - val_accuracy: 0.9808\n",
            "Epoch 36/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0387 - accuracy: 0.9869 - val_loss: 0.0872 - val_accuracy: 0.9793\n",
            "Epoch 37/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0369 - accuracy: 0.9883 - val_loss: 0.0896 - val_accuracy: 0.9797\n",
            "Epoch 38/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0357 - accuracy: 0.9881 - val_loss: 0.0898 - val_accuracy: 0.9803\n",
            "Epoch 39/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0367 - accuracy: 0.9876 - val_loss: 0.0871 - val_accuracy: 0.9803\n",
            "Epoch 40/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0348 - accuracy: 0.9888 - val_loss: 0.0971 - val_accuracy: 0.9781\n",
            "Epoch 41/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0359 - accuracy: 0.9879 - val_loss: 0.0864 - val_accuracy: 0.9803\n",
            "Epoch 42/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0328 - accuracy: 0.9892 - val_loss: 0.0877 - val_accuracy: 0.9796\n",
            "Epoch 43/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0331 - accuracy: 0.9892 - val_loss: 0.0840 - val_accuracy: 0.9805\n",
            "Epoch 44/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0329 - accuracy: 0.9887 - val_loss: 0.0906 - val_accuracy: 0.9793\n",
            "Epoch 45/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0321 - accuracy: 0.9890 - val_loss: 0.0944 - val_accuracy: 0.9797\n",
            "Epoch 46/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0338 - accuracy: 0.9886 - val_loss: 0.0918 - val_accuracy: 0.9796\n",
            "Epoch 47/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0349 - accuracy: 0.9879 - val_loss: 0.0934 - val_accuracy: 0.9796\n",
            "Epoch 48/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0304 - accuracy: 0.9901 - val_loss: 0.0905 - val_accuracy: 0.9803\n",
            "Epoch 49/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0332 - accuracy: 0.9893 - val_loss: 0.0910 - val_accuracy: 0.9797\n",
            "Epoch 50/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0317 - accuracy: 0.9896 - val_loss: 0.0942 - val_accuracy: 0.9792\n",
            "Epoch 51/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0301 - accuracy: 0.9899 - val_loss: 0.0875 - val_accuracy: 0.9802\n",
            "Epoch 52/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.0881 - val_accuracy: 0.9803\n",
            "Epoch 53/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0306 - accuracy: 0.9898 - val_loss: 0.0966 - val_accuracy: 0.9790\n",
            "Epoch 54/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0307 - accuracy: 0.9900 - val_loss: 0.0889 - val_accuracy: 0.9797\n",
            "Epoch 55/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0313 - accuracy: 0.9893 - val_loss: 0.0949 - val_accuracy: 0.9803\n",
            "Epoch 56/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0321 - accuracy: 0.9886 - val_loss: 0.0896 - val_accuracy: 0.9800\n",
            "Epoch 57/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0291 - accuracy: 0.9899 - val_loss: 0.0919 - val_accuracy: 0.9797\n",
            "Epoch 58/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.0888 - val_accuracy: 0.9808\n",
            "Epoch 59/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0298 - accuracy: 0.9906 - val_loss: 0.0913 - val_accuracy: 0.9794\n",
            "Epoch 60/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0977 - val_accuracy: 0.9803\n",
            "Epoch 61/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0265 - accuracy: 0.9912 - val_loss: 0.0935 - val_accuracy: 0.9802\n",
            "Epoch 62/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0274 - accuracy: 0.9912 - val_loss: 0.1002 - val_accuracy: 0.9797\n",
            "Epoch 63/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0283 - accuracy: 0.9906 - val_loss: 0.0948 - val_accuracy: 0.9787\n",
            "Epoch 64/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0294 - accuracy: 0.9902 - val_loss: 0.0936 - val_accuracy: 0.9798\n",
            "Epoch 65/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0274 - accuracy: 0.9906 - val_loss: 0.1073 - val_accuracy: 0.9793\n",
            "Epoch 66/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0267 - accuracy: 0.9915 - val_loss: 0.1043 - val_accuracy: 0.9791\n",
            "Epoch 67/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.1012 - val_accuracy: 0.9806\n",
            "Epoch 68/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0279 - accuracy: 0.9904 - val_loss: 0.1034 - val_accuracy: 0.9799\n",
            "Epoch 69/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.1115 - val_accuracy: 0.9787\n",
            "Epoch 70/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0272 - accuracy: 0.9908 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 71/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0268 - accuracy: 0.9910 - val_loss: 0.1058 - val_accuracy: 0.9811\n",
            "Epoch 72/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0279 - accuracy: 0.9907 - val_loss: 0.0976 - val_accuracy: 0.9798\n",
            "Epoch 73/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0244 - accuracy: 0.9914 - val_loss: 0.1028 - val_accuracy: 0.9810\n",
            "Epoch 74/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 0.1049 - val_accuracy: 0.9801\n",
            "Epoch 75/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 0.1010 - val_accuracy: 0.9793\n",
            "Epoch 76/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.1027 - val_accuracy: 0.9794\n",
            "Epoch 77/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 0.1044 - val_accuracy: 0.9799\n",
            "Epoch 78/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.1031 - val_accuracy: 0.9795\n",
            "Epoch 79/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0256 - accuracy: 0.9916 - val_loss: 0.1057 - val_accuracy: 0.9805\n",
            "Epoch 80/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.1027 - val_accuracy: 0.9796\n",
            "Epoch 81/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.1056 - val_accuracy: 0.9806\n",
            "Epoch 82/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0252 - accuracy: 0.9915 - val_loss: 0.1066 - val_accuracy: 0.9796\n",
            "Epoch 83/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 0.9922 - val_loss: 0.0970 - val_accuracy: 0.9810\n",
            "Epoch 84/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.1048 - val_accuracy: 0.9805\n",
            "Epoch 85/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9930 - val_loss: 0.1073 - val_accuracy: 0.9804\n",
            "Epoch 86/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9921 - val_loss: 0.1125 - val_accuracy: 0.9786\n",
            "Epoch 87/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9930 - val_loss: 0.1038 - val_accuracy: 0.9811\n",
            "Epoch 88/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9919 - val_loss: 0.1072 - val_accuracy: 0.9797\n",
            "Epoch 89/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.1001 - val_accuracy: 0.9793\n",
            "Epoch 90/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.1041 - val_accuracy: 0.9793\n",
            "Epoch 91/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.1068 - val_accuracy: 0.9803\n",
            "Epoch 92/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0214 - accuracy: 0.9929 - val_loss: 0.1059 - val_accuracy: 0.9804\n",
            "Epoch 93/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.1041 - val_accuracy: 0.9799\n",
            "Epoch 94/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.1113 - val_accuracy: 0.9806\n",
            "Epoch 95/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9921 - val_loss: 0.1082 - val_accuracy: 0.9800\n",
            "Epoch 96/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0219 - accuracy: 0.9930 - val_loss: 0.1112 - val_accuracy: 0.9793\n",
            "Epoch 97/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.1194 - val_accuracy: 0.9789\n",
            "Epoch 98/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.1053 - val_accuracy: 0.9803\n",
            "Epoch 99/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.1071 - val_accuracy: 0.9793\n",
            "Epoch 100/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.1148 - val_accuracy: 0.9791\n",
            "Epoch 101/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - accuracy: 0.9931 - val_loss: 0.1104 - val_accuracy: 0.9805\n",
            "Epoch 102/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9928 - val_loss: 0.1097 - val_accuracy: 0.9801\n",
            "Epoch 103/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.1165 - val_accuracy: 0.9796\n",
            "Epoch 104/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9936 - val_loss: 0.1091 - val_accuracy: 0.9798\n",
            "Epoch 105/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.1217 - val_accuracy: 0.9784\n",
            "Epoch 106/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.1088 - val_accuracy: 0.9791\n",
            "Epoch 107/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9935 - val_loss: 0.1086 - val_accuracy: 0.9805\n",
            "Epoch 108/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9936 - val_loss: 0.1138 - val_accuracy: 0.9794\n",
            "Epoch 109/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9931 - val_loss: 0.1088 - val_accuracy: 0.9804\n",
            "Epoch 110/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.1129 - val_accuracy: 0.9801\n",
            "Epoch 111/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.1143 - val_accuracy: 0.9807\n",
            "Epoch 112/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9927 - val_loss: 0.1150 - val_accuracy: 0.9797\n",
            "Epoch 113/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.1004 - val_accuracy: 0.9803\n",
            "Epoch 114/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9934 - val_loss: 0.1105 - val_accuracy: 0.9803\n",
            "Epoch 115/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.1095 - val_accuracy: 0.9817\n",
            "Epoch 116/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.1161 - val_accuracy: 0.9797\n",
            "Epoch 117/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.1091 - val_accuracy: 0.9797\n",
            "Epoch 118/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9931 - val_loss: 0.1054 - val_accuracy: 0.9808\n",
            "Epoch 119/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1116 - val_accuracy: 0.9812\n",
            "Epoch 120/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9929 - val_loss: 0.1138 - val_accuracy: 0.9809\n",
            "Epoch 121/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1171 - val_accuracy: 0.9803\n",
            "Epoch 122/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 0.1223 - val_accuracy: 0.9796\n",
            "Epoch 123/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.1091 - val_accuracy: 0.9805\n",
            "Epoch 124/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 0.1080 - val_accuracy: 0.9793\n",
            "Epoch 125/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.1135 - val_accuracy: 0.9791\n",
            "Epoch 126/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.1138 - val_accuracy: 0.9800\n",
            "Epoch 127/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9935 - val_loss: 0.1094 - val_accuracy: 0.9805\n",
            "Epoch 128/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.1180 - val_accuracy: 0.9797\n",
            "Epoch 129/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 0.1124 - val_accuracy: 0.9808\n",
            "Epoch 130/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - accuracy: 0.9931 - val_loss: 0.1179 - val_accuracy: 0.9800\n",
            "Epoch 131/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 0.1145 - val_accuracy: 0.9797\n",
            "Epoch 132/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0206 - accuracy: 0.9938 - val_loss: 0.1131 - val_accuracy: 0.9803\n",
            "Epoch 133/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9932 - val_loss: 0.1085 - val_accuracy: 0.9804\n",
            "Epoch 134/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.1059 - val_accuracy: 0.9808\n",
            "Epoch 135/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.1090 - val_accuracy: 0.9801\n",
            "Epoch 136/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.1112 - val_accuracy: 0.9809\n",
            "Epoch 137/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.1118 - val_accuracy: 0.9809\n",
            "Epoch 138/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.1115 - val_accuracy: 0.9810\n",
            "Epoch 139/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.1067 - val_accuracy: 0.9811\n",
            "Epoch 140/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.1112 - val_accuracy: 0.9804\n",
            "Epoch 141/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9936 - val_loss: 0.1073 - val_accuracy: 0.9793\n",
            "Epoch 142/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1155 - val_accuracy: 0.9797\n",
            "Epoch 143/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.1137 - val_accuracy: 0.9801\n",
            "Epoch 144/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.1180 - val_accuracy: 0.9808\n",
            "Epoch 145/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 0.1132 - val_accuracy: 0.9812\n",
            "Epoch 146/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.1124 - val_accuracy: 0.9795\n",
            "Epoch 147/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9941 - val_loss: 0.1185 - val_accuracy: 0.9795\n",
            "Epoch 148/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1113 - val_accuracy: 0.9799\n",
            "Epoch 149/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9943 - val_loss: 0.1163 - val_accuracy: 0.9798\n",
            "Epoch 150/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.1081 - val_accuracy: 0.9812\n",
            "Epoch 151/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.1168 - val_accuracy: 0.9803\n",
            "Epoch 152/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9943 - val_loss: 0.1128 - val_accuracy: 0.9801\n",
            "Epoch 153/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.1146 - val_accuracy: 0.9793\n",
            "Epoch 154/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.1222 - val_accuracy: 0.9796\n",
            "Epoch 155/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.1181 - val_accuracy: 0.9801\n",
            "Epoch 156/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 0.1134 - val_accuracy: 0.9801\n",
            "Epoch 157/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.1139 - val_accuracy: 0.9799\n",
            "Epoch 158/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9941 - val_loss: 0.1188 - val_accuracy: 0.9787\n",
            "Epoch 159/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 0.1084 - val_accuracy: 0.9814\n",
            "Epoch 160/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.1124 - val_accuracy: 0.9808\n",
            "Epoch 161/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9938 - val_loss: 0.1215 - val_accuracy: 0.9793\n",
            "Epoch 162/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1109 - val_accuracy: 0.9801\n",
            "Epoch 163/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1156 - val_accuracy: 0.9795\n",
            "Epoch 164/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.1176 - val_accuracy: 0.9801\n",
            "Epoch 165/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.1185 - val_accuracy: 0.9808\n",
            "Epoch 166/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.1176 - val_accuracy: 0.9793\n",
            "Epoch 167/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9940 - val_loss: 0.1178 - val_accuracy: 0.9803\n",
            "Epoch 168/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.1147 - val_accuracy: 0.9797\n",
            "Epoch 169/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.1154 - val_accuracy: 0.9812\n",
            "Epoch 170/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.1231 - val_accuracy: 0.9802\n",
            "Epoch 171/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.1193 - val_accuracy: 0.9797\n",
            "Epoch 172/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0151 - accuracy: 0.9947 - val_loss: 0.1165 - val_accuracy: 0.9805\n",
            "Epoch 173/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.1143 - val_accuracy: 0.9806\n",
            "Epoch 174/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9948 - val_loss: 0.1236 - val_accuracy: 0.9807\n",
            "Epoch 175/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.1235 - val_accuracy: 0.9807\n",
            "Epoch 176/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.1231 - val_accuracy: 0.9803\n",
            "Epoch 177/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.1120 - val_accuracy: 0.9802\n",
            "Epoch 178/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.1211 - val_accuracy: 0.9803\n",
            "Epoch 179/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.1217 - val_accuracy: 0.9797\n",
            "Epoch 180/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1170 - val_accuracy: 0.9799\n",
            "Epoch 181/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.1326 - val_accuracy: 0.9793\n",
            "Epoch 182/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.1176 - val_accuracy: 0.9811\n",
            "Epoch 183/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.1176 - val_accuracy: 0.9799\n",
            "Epoch 184/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.1289 - val_accuracy: 0.9806\n",
            "Epoch 185/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.1242 - val_accuracy: 0.9798\n",
            "Epoch 186/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.1193 - val_accuracy: 0.9804\n",
            "Epoch 187/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.1186 - val_accuracy: 0.9790\n",
            "Epoch 188/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.1281 - val_accuracy: 0.9803\n",
            "Epoch 189/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.1284 - val_accuracy: 0.9800\n",
            "Epoch 190/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 0.9946 - val_loss: 0.1278 - val_accuracy: 0.9790\n",
            "Epoch 191/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9943 - val_loss: 0.1235 - val_accuracy: 0.9795\n",
            "Epoch 192/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9949 - val_loss: 0.1172 - val_accuracy: 0.9800\n",
            "Epoch 193/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.1150 - val_accuracy: 0.9792\n",
            "Epoch 194/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.1099 - val_accuracy: 0.9807\n",
            "Epoch 195/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0147 - accuracy: 0.9952 - val_loss: 0.1214 - val_accuracy: 0.9797\n",
            "Epoch 196/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.1178 - val_accuracy: 0.9801\n",
            "Epoch 197/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.1183 - val_accuracy: 0.9805\n",
            "Epoch 198/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0158 - accuracy: 0.9946 - val_loss: 0.1355 - val_accuracy: 0.9792\n",
            "Epoch 199/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.1243 - val_accuracy: 0.9805\n",
            "Epoch 200/200\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.1283 - val_accuracy: 0.9811\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feae0385278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta70kS1yXMza",
        "colab_type": "code",
        "outputId": "2e1143c2-4096-4827-f75f-8630c646cc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1370 - accuracy: 0.9796\n",
            "Test accuracy: 0.9796000123023987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1cK2xrBaSUA",
        "colab_type": "code",
        "outputId": "c328d1c0-439b-4250-de51-9ad0202eebaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAIjCAYAAAD2niXtAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nYRCRMgYGESljYBCRMm8Anzq7CCJyD/8fD9CHfXbPK4YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2OwrPbquEw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('./my_modeldropadam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8GRYBKqwRqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\t\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH9ZcDs0wtrv",
        "colab_type": "code",
        "outputId": "536c387a-c1d2-436e-a6e5-b4d671b72f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "os.getcwd()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqGetsFew6l-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('my_modeldropadam.h5', save_format='h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngmEssOkyguz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save_weights('my_model.h5', save_format='h5')\n",
        "# model.load_weights(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGfzncb2yp3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#json_string = model.to_json()  # save\n",
        "#model = tf.keras.models.model_from_json(json_string) # restore\n",
        "#yaml_string = model.to_yaml() # save\n",
        "#model = tf.keras.models.model_from_yaml(yaml_string) # restore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNzp8sOJy1Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_string = model.to_json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLWZb7F0y6tD",
        "colab_type": "code",
        "outputId": "15a28317-7f89-4d9f-9322-eb60813077f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(json_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_layer\", \"trainable\": true, \"batch_input_shape\": [null, 784], \"dtype\": \"float32\", \"units\": 128, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0.3, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_layer_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 128, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0.3, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_layer_3\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 10, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 784]}, \"keras_version\": \"2.3.0-tf\", \"backend\": \"tensorflow\"}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}